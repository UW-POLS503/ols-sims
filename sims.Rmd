---
title: OLS Simulations
author: Jeffrey Arnold
date: April 12, 2017
---

$$
\DeclareMathOperator{diag}{diag}
$$

# Prerequisites

Before starting, load the necessary packages and define some functions that will be used later.

```{r message=FALSE}
library("tidyverse")
library("stringr")
library("broom")
library("modelr")
```

Combine standard deviation and correlation to make a covariance function.
```{r}
sdcor2cov <- function(s, r = diag(length(s))) {
  s <- diag(s, nrow = length(s), ncol = length(s))
  s %*% r %*% s
}
```
The $n \times n$ covariance matrix $\Sigma$ can be decomposed into the $n \times 1$ standard deviation vector $\sigma$ and correlation matrix $R$,
$$
\Sigma = \diag(\sigma) R \diag(\sigma)
$$

Generate a data frame with possibly correlated, multivariate normal variables.
The multivariate normal distribution is specified in terms of a mean (`mu`), 
standard deviation for variable (`sigma`), and the correlation matrix between variables,
(`R`):
```{r}
mvnorm_df <- function(n, mu, sigma = rep(1, length(mu)), R = diag(length(sd)),
                      empirical = TRUE) {
  as_tibble(MASS::mvrnorm(n, mu = mu, Sigma = sdcor2cov(sigma, R), empirical = empirical))
}
```

Find a regression standard error that will produce a given $R^2$.
For a normal linear model, given a sample $\hat{y}$ find the regression standard error, $\sigma^2$ to produce the desired $R^2$.
```{r }
r2_to_sigma <- function(y, r2) {
  # Var(Y) = E(Var(Y|X)) + Var(E(Y|X))
  ssm <- sum((y - mean(y)) ^ 2)
  sse <- (1 - r2) / r2 * ssm
  # return sigma (assume population n)
  sqrt(sse / n)
}
```


# Classical Model

$$
\begin{aligned}[t]
\vec{y} &= \mat{X} \vec{\beta} + \vec{\epsilon} \\
\epsilon_i &\sim N(0, \sigma^2)
\end{aligned}
$$

The following function will simulate data from a linear model with i.i.d. normally distributed errors.

- `.data`: Data frame with covariates in $X$
- `beta`: Coefficients
- `sigma`: Standard error of the regression
- `ov`: Any variables omitted when estimating OLS

```{r}
sim_lm_normal <- function(.data, beta, sigma, ov = NULL) {
  n <- nrow(.data)
  # Generate the formula to estimate (potentially removing variables)
  f <- paste0("y", "~", paste0(setdiff(names(.data), ov), collapse = "+"))
  # generate and add y variable
  E_y <- cbind(1, as.matrix(.data)) %*% beta
  eps <- rnorm(n, mean = 0, sd = sigma)
  .data$y <- E_y + eps
  # Do not save model.frame
  mod <- lm(f, data = .data, model = FALSE)
  # keep only the coefficients and their standard errors
  tidy(mod)[ , c("term", "estimate", "std.error")]
}
```

```{r}
n <- 100
k <- 1
beta <- c(0, rep(1, k))
mu <- rep(0, k)
X <- mvnorm_df(n, rep(0, k))
E_y <- cbind(1, as.matrix(X)) %*% beta
sigma_y <- r2_to_sigma(E_y, 0.5)
```

Run `sim_lm_normal` with these parameters `n_sims` times:
```{r}
n_sims <- 2048
res_sim1 <- map_df(seq_len(n_sims), function(i) sim_lm_normal(X, beta, sigma_y), .id = ".id")
```

For these simulations, calculate the bias of the regression coefficients and standard error of the regression coefficient:
```{r}
res_sim1 %>%
  filter(term != "(Intercept)") %>%
  summarise(beta_hat_mean = mean(estimate),
            beta_hat_sd = sd(estimate),
            se_mean = mean(std.error),
            se_sd = sd(std.error)) %>%
  mutate(beta_hat_bias = (beta_hat_mean - beta[-1]),
         se_bias = (se_mean - beta_hat_sd))

```

Now rerun `sim_lm_normal` simultions, but vary the size of the sample
```{r}
sample_size <- c(8, 64, 512, 4096)
```

For each of these sample size, we will run `r n_sims` simulations, and save the results in a data fram along with the sample size:
```{r}
run_sim1a <- function(n) {
  X <- mvnorm_df(n, mu = rep(0, k))
  out <- map_df(seq_len(n_sims), 
                function(i) sim_lm_normal(X, beta, sigma_y), .id = ".id")
  out$n <- n
  out
}
```
Now run the above function for each sample size and return the results as a data frame with all simulations and sample sizes:
```{r}
res_sim1a <- map_df(sample_size, run_sim1a)
```

```{r}
res_sim1a %>%
  filter(term != "(Intercept)") %>%
  group_by(n) %>%
  summarise(beta_hat_mean = mean(estimate),
            beta_hat_sd = sd(estimate),
            se_mean = mean(std.error),
            se_sd = sd(std.error)) %>%
  mutate(beta_hat_bias = (beta_hat_mean - beta[-1]),
         se_bias = (se_mean - beta_hat_sd))
```


# Random X

In the previous example, the data, $X$ are fixed. What happens if $X$ is random? 

```{r}
# Simulate from linear model model
# random X uncorrelated with Y
sim_lm_random_X <- function(n, mu_X, sigma_X, R_X, beta, sigma_y, ov = NULL) {
  .data <- mvnorm_df(n, mu = mu_X, sigma = sigma_X, R = R_X,
  # Generate the formula to estimate (potentially removing variables)
  f <- paste0("y", "~", paste0(setdiff(names(.data), ov), collapse = "+"))
  # generate and add y variable
  E_y <- cbind(1, as.matrix(.data)) %*% beta
  eps <- rnorm(n, mean = 0, sd = sigma)
  .data$y <- E_y + eps
  lm(f, data = .data, model = FALSE)
}
```

**TODO**

# Non-normal distribution

What about if the errors are i.i.d, but generated from a non-normal distribution?

There are two issues with the error distribution that will be considered:

1. it is skewed 
2. it has wider tails than a normal distribution

We will handle both of these by considering errors generated from a skew t-distribution.
The skew t-distribution is an extension of the Student's t-distribution that allows for skewness.

- `df` = degrees of freedom, as in Student's t distribution When `df = Inf` it is the normal distribution, as `df` decreases, the tails of the distribution are wider than the normal
- `xi` is the skewnewss parameter. If it is in (0, 1) it is left skew, if it is 1 then it is symmetric, and if `xi > 1` it is right skewed.

```{r}
sim_skewt <- function(.data, beta, sigma, df = 5, skew = 1.5, ov = NULL) {
  # if ov is not specified use the last variable in .datas
  ov <- ov %||% names(.data)[ncol(.data)]
  n <- nrow(.data)
  E_y <- cbind(1, as.matrix(.data)) %*% beta
  eps <- fGarch::rsstd(n, mean = 0, sd = sigma, nu = df, xi = skew)
  .data$y <- E_y + eps
  # generate linear regression formula - which excludes the
  # omitted variable
  f <- paste0("y", "~", paste0(setdiff(names(.data), ov), collapse = "+"))
  lm(f, data = .data, model = FALSE)
}
```

# Non-constant errors 

What if the errors are not constant? This can be done using `sim_lin_normal` by passing a vector to `sigma`.



# Correlated errors

## Clustered

What if the errors are clustered? This means that within each group the errors are correlated.


```{r}
sim_cluster <- function(.data, groups, beta, sigma_y, sigma_g) {
  n <- nrow(.data)
  g <- cut_number(seq_len(n), groups, labels = false)
  E_y <- cbind(1, as.matrix(.data)) %*% beta
  # individual random error
  eps <- rnorm(n, mean = 0, sd = sigma_y)
  # group error
  nu <- rnorm(groups, mean = 0, sd = sigma_g)
  .data$y <- E_y + nu[g] + eps
  # omitted variable
  f <- paste0("y", "~", paste0(names(.data), collapse = "+"))
  lm(f, data = .data, model.frame = FALSE, y = FALSE, X = FALSE)
}
```

## Serial Correlation

What if the errors are correlated over time? 

```{r}
sim_armax <- function(.data, groups, beta, sigma_y, ar = NULL, ma = NULL) {
  n <- nrow(.data)
  E_y <- cbind(1, as.matrix(.data)) %*% beta
  # ARMA
  eps <- arima.sim(list(ar = ar, ma = ma), n, sd = sigma_y)
  # group error
  .data$y <- E_y + eps
  # omitted variable
  f <- paste0("y", "~", paste0(names(.data), collapse = "+"))
  lm(f, data = .data, model.frame = FALSE, y = FALSE, X = FALSE)
}
```


## Measurement Error

```{r}
sim_measure_error <- function(.data, beta, sigma_y, rho) {
  n <- nrow(.data)
  E_y <- cbind(1, as.matrix(.data)) %*% beta
  eps <- rnorm(n, mean = 0, sd = sigma_y)
  y <- E_y + eps
  # add measurement error before the regression
  # Need to add *after* y is generated
  # easier to do this without adding rho
  for (i in seq_along(rho)) {
    # scale of measurement error
    # rho = delta^2 /  (delta^2 + var(x)^2)
    # rho = 0 is no measurement error
    # rho = 1 is all measurement error
    delta <- sqrt(rho[[i]] / (1 - rho[[i]]) * var[[.data[[i]]]])
    .data[[i]] <- .data[[i]] + rnorm(n, mean = 0, sd = delta)
  }
  # add y to the data frame
  .data$y <- y
  f <- paste0("y", "~", paste0(names(.data), collapse = "+"))
  lm(f, data = .data, model = FALSE)
}
```



# Selection on Outcome and Predictors

## Selection on the Outcome

What if the sample is selected based on the value of $y$? This is often called "selection on the dependent variable"?

```{r} 
sim_select_y <- function(.data, beta, sigma_y, ub = 1, lb = 0) {
  n <- nrow(.data)
  E_y <- cbind(1, as.matrix(.data)) %*% beta
  eps <- rnorm(n, mean = 0, sd = sigma_y)
  y <- E_y + eps
  # add y to the data frame
  .data$y <- y
  # remove all y values < quantile or greater than quantile
  .data <- dplyr::filter(.data,
                         y >= quantile(y, lb),
                         y <= quantile(y, ub))
  f <- paste0("y", "~", paste0(names(.data), collapse = "+"))
  lm(f, data = .data, model.frame = FALSE, y = FALSE, X = FALSE)
}
```

## Selection on Predictors

```{r}
sim_select_x <- function(.data, beta, sigma_y, ub = 1, lb = 0) {
  n <- nrow(.data)
  E_y <- cbind(1, as.matrix(.data)) %*% beta
  eps <- rnorm(n, mean = 0, sd = sigma_y)
  y <- E_y + eps
  # remove all x values < quantile or greater than quantile
  tokeep <- rep(TRUE, n)
  # but calculate all bounds prior to filtering
  if (length(ub) == 1) {
    ub <- rep(ub, ncol(.data))
  }
  if (length(lb) == 1) {
    lb <- rep(lb, ncol(.data))
  }
  for (i in seq_along(ub)) {
    tokeep <- (tokeep &
               .data[[i]] >= quantile(.data[[i]], lb) &
               .data[[i]] <= quantile(.data[[i]], ub))
  }
  # remove bad rows
  .data <- .data[keep, , drop = FALSE]
  # add y to the data frame
  .data$y <- y
  f <- paste0("y", "~", paste0(names(.data), collapse = "+"))
  lm(f, data = .data, model = FALSE)
}
```

